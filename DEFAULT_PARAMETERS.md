# ‚öôÔ∏è Default Parameter Values - Complete Reference

## All Current Default Settings

---

## üöÄ Main Training Parameters

### **Parallelization:**
```python
n_workers = 100              # Number of parallel worker processes
```

### **Episode Budget:**
```python
total_episode_budget = 100000           # Total episodes across all workflows
max_train_episodes_per_workflow = 10000 # Max episodes per workflow (hard cap)
episodes_per_update = 100               # Episodes collected before each PPO update
```

### **Environment:**
```python
red_agent = 'B_lineAgent'    # Red agent type (B_lineAgent, RedMeanderAgent, or SleepAgent)
max_steps = 100              # Max steps per episode
scenario_path = '/home/ubuntu/CAGE2/cage-challenge-2/CybORG/CybORG/Shared/Scenarios/Scenario2.yaml'
```

---

## üéØ Compliance & Reward Parameters

### **Compliance:**
```python
compliance_threshold = 0.90  # Target compliance rate (changed from 0.95 to 0.90)
patience_updates = 12        # Stop if no improvement for this many updates
```

### **Alignment Reward:**
```python
alignment_lambda = 30.0             # Weight for compliance reward (linear scaling)
compliant_bonus_scale = 0.0         # Additional per-step bonus for compliant actions
violation_penalty_scale = 0.0       # Penalty for non-compliant actions
compliance_focus_weight = 75.0      # (Currently unused)
```

**Alignment Bonus Calculation:**
```python
# Simple linear scaling (no exponential boost)
alignment_bonus = alignment_lambda √ó compliance_rate
                = 30.0 √ó 0.75
                = 22.5
```

---

## üß† PPO Hyperparameters

### **Neural Network:**
```python
K_epochs = 6                 # Number of PPO epochs per update (fixed)
lr = 0.002                   # Learning rate (Adam optimizer)
eps_clip = 0.2               # PPO clipping parameter
gamma = 0.99                 # Discount factor for returns
betas = [0.9, 0.990]         # Adam optimizer betas
```

### **Network Architecture:**
```python
input_dims = 52              # State observation dimensions
order_dims = 25              # Workflow encoding dimensions (5√ó5 one-hot)
n_actions = 145              # Full CAGE2 action space
hidden_dims = 64             # Hidden layer size
```

**Network Structure:**
```
Input: [State (52) + Workflow (25)] = 77 dimensions
  ‚Üì
Actor: 77 ‚Üí 64 ‚Üí 64 ‚Üí 145 (actions)
Critic: 77 ‚Üí 64 ‚Üí 64 ‚Üí 1 (value)
```

---

## üîç GP-UCB Search Parameters

### **Gaussian Process:**
```python
gp_beta = 2.0                # Exploration parameter for UCB
                             # UCB = mean + beta √ó std
```

### **Workflow Selection:**
```python
n_candidate_workflows = 120  # Evaluate ALL permutations (5! = 120)
```

---

## üìä Complete Parameter Summary

| Category | Parameter | Default Value | Unit |
|----------|-----------|---------------|------|
| **Workers** | n_workers | 100 | workers |
| **Budget** | total_episode_budget | 100,000 | episodes |
| **Budget** | max_episodes_per_workflow | 10,000 | episodes |
| **Budget** | episodes_per_update | 100 | episodes |
| **Compliance** | compliance_threshold | **0.90** | rate |
| **Compliance** | patience_updates | 12 | updates |
| **Reward** | alignment_lambda | 30.0 | weight |
| **Reward** | compliant_bonus_scale | 0.0 | weight |
| **Reward** | violation_penalty_scale | 0.0 | weight |
| **PPO** | K_epochs | 6 | epochs |
| **PPO** | learning_rate | 0.002 | - |
| **PPO** | eps_clip | 0.2 | - |
| **PPO** | gamma | 0.99 | - |
| **GP-UCB** | beta | 2.0 | - |
| **Environment** | red_agent | B_lineAgent | class |
| **Environment** | max_steps | 100 | steps |

---

## üéÆ Command Line Options

### **All Available Arguments:**

```bash
python workflow_rl/executor_async_train_workflow_rl.py \
    --n-workers 100 \
    --total-episodes 100000 \
    --max-episodes-per-workflow 10000 \
    --episodes-per-update 100 \
    --red-agent B_lineAgent \
    --alignment-lambda 30.0 \
    --compliance-threshold 0.90
```

### **Common Configurations:**

#### **Quick Test:**
```bash
--n-workers 10 \
--total-episodes 1000 \
--max-episodes-per-workflow 500 \
--episodes-per-update 50
```

#### **Production (Default):**
```bash
--n-workers 100 \
--total-episodes 100000 \
--compliance-threshold 0.90  # Now 90% instead of 95%
```

#### **High Compliance Focus:**
```bash
--alignment-lambda 50.0 \
--compliance-threshold 0.95
```

---

## üìù Key Changes from Original

### **What Changed:**

| Parameter | Original | Current | Reason |
|-----------|----------|---------|--------|
| **compliance_threshold** | 0.95 | **0.90** | Easier to achieve |
| **alignment_lambda** | 30.0 | **30.0** | Reverted (no exponential) |
| **K_epochs** | 6 (adaptive) | **6 (fixed)** | Simplified |
| **Alignment scaling** | Exponential | **Linear** | Back to original |

### **What Stayed the Same:**

- ‚úÖ All 120 workflows evaluated
- ‚úÖ Workflow-specific policy storage
- ‚úÖ Correct compliance calculation (true state checking)
- ‚úÖ ProcessPoolExecutor async architecture
- ‚úÖ Three-component reward logging

---

## üí° Effect of 90% Threshold

### **Before (95% threshold):**
```
Workflows reaching threshold: ~20%
Avg plateau compliance: 75-85%
Many workflows hit early stopping
```

### **After (90% threshold):**
```
Expected workflows reaching threshold: ~60-70%
Avg plateau compliance: still 75-85%
But MORE workflows achieve 90% before plateau
Faster workflow search (less training per workflow)
```

---

## üéØ Recommended Settings for Different Goals

### **Fast Exploration (Try Many Workflows):**
```python
compliance_threshold = 0.80           # Lower bar
max_episodes_per_workflow = 2000      # Shorter training
episodes_per_update = 50              # More frequent updates
```

### **High Quality (Best Workflows):**
```python
compliance_threshold = 0.95           # High bar
max_episodes_per_workflow = 10000     # Longer training
alignment_lambda = 50.0               # Higher weight on compliance
```

### **Balanced (Current Default):**
```python
compliance_threshold = 0.90           # Moderate bar
max_episodes_per_workflow = 10000     # Allow full training
alignment_lambda = 30.0               # Standard weight
```

---

## üîß How to Override Defaults

### **Method 1: Command Line**
```bash
python workflow_rl/executor_async_train_workflow_rl.py \
    --compliance-threshold 0.85 \
    --alignment-lambda 40.0
```

### **Method 2: Edit Script**
```python
# In executor_async_train_workflow_rl.py, line 822-823
parser.add_argument('--alignment-lambda', type=float, default=50.0)  # Change here
parser.add_argument('--compliance-threshold', type=float, default=0.85)  # Change here
```

### **Method 3: Edit run_executor_async_training.sh**
```bash
python workflow_rl/executor_async_train_workflow_rl.py \
    --compliance-threshold 0.85 \  # Add this line
    --alignment-lambda 40.0 \      # Add this line
    ...
```

---

## ‚úÖ Summary

**Current defaults provide:**
- ‚úÖ 90% compliance threshold (achievable for most workflows)
- ‚úÖ Linear alignment rewards (simple, no exponential)
- ‚úÖ Fixed 6 PPO epochs (consistent)
- ‚úÖ 100 workers (maximum performance)
- ‚úÖ 100k episode budget (thorough exploration)

**This configuration balances:**
- Speed: Most workflows reach 90% in 2000-4000 episodes
- Quality: 90% compliance is still very good
- Exploration: Can try more workflows with same budget
- Simplicity: No complex reward scaling
