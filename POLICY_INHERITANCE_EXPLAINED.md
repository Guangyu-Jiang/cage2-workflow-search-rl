# ğŸ”„ Workflow-Specific Policy Storage

## Policy is Saved Per Workflow!

The system stores a **separate policy for each unique workflow**. When you revisit the same workflow, it resumes from that workflow's previous policy. **Brand-new workflows now warm-start from the closest previously trained workflow (by Kendall distance)** so they never lose the defensive knowledge we have already acquired.

---

## ğŸ“ How It Works

### **First Time Training a Workflow (with warm-start):**
```python
workflow_key = tuple(workflow_order)

if workflow_key in self.workflow_policies:
    # This workflow was trained before
    pass
else:
    # NEW workflow - initialize from nearest neighbour
    print("  Creating new agent (new workflow - training from scratch)")
    agent = ParallelOrderConditionedPPO(...)
    closest_key, closest_agent, closest_distance = self._find_closest_trained_workflow(workflow_order)
    if closest_agent is not None:
        closest_order = ' â†’ '.join(list(closest_key))
        print(f"  Initializing from closest trained workflow: {closest_order} (dist={closest_distance:.3f})")
        agent.policy.load_state_dict(closest_agent.policy.state_dict())
```

**Output:**
```
Iteration 1
  Selected: user â†’ op_host â†’ op_server â†’ enterprise â†’ defender
  Creating new agent (new workflow - training from scratch)
  Initializing from closest trained workflow: defender â†’ op_server â†’ enterprise â†’ op_host â†’ user (dist=0.20)
```

### **Revisiting the Same Workflow:**
```python
if workflow_key in self.workflow_policies:
    # This EXACT workflow was trained before!
    print("  Inheriting policy from previous training of THIS workflow")
    agent = ParallelOrderConditionedPPO(...)
    
    # Load weights from THIS workflow's previous training
    agent.policy.load_state_dict(self.workflow_policies[workflow_key].policy.state_dict())
```

**Output:**
```
Iteration 15
  Selected: user â†’ op_host â†’ op_server â†’ enterprise â†’ defender  (same as Iteration 1!)
  Inheriting policy from previous training of THIS workflow
  (Resuming from checkpoint for this specific workflow)
```

### **After Training:**
```python
# Save agent for THIS specific workflow
workflow_key = tuple(workflow_order)
self.workflow_policies[workflow_key] = agent
```

---

## ğŸ¯ Visual Flow (New Behavior)

```
Iteration 1: Workflow A (user â†’ op_host â†’ op_server â†’ enterprise â†’ defender)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ New Agent        â”‚
â”‚ (Random Init)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Train from scratch
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Policy_A   â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ Save in workflow_policies[A]
          â–¼
          
Iteration 2: Workflow B (defender â†’ enterprise â†’ op_server â†’ op_host â†’ user)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ New Agent        â”‚
â”‚ (Warm Start)     â”‚  â† NEW workflow, initialized from nearest neighbour!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Fine-tune from inherited weights
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Policy_B   â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ Save in workflow_policies[B]
          â–¼
          
Iteration 3: Workflow A (user â†’ op_host â†’ op_server â†’ enterprise â†’ defender)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load from        â”‚
â”‚ Policy_A         â”‚â†â”€â”€ Same as Iteration 1! Inherit from Policy_A
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Resume training (fine-tune)
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Policy_A   â”‚
    â”‚ (updated)  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ Update workflow_policies[A]
          â–¼

Iteration 4: Workflow C (op_server â†’ defender â†’ ...)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ New Agent        â”‚
â”‚ (Warm Start)     â”‚  â† NEW workflow again!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Fine-tune from inherited weights
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Policy_C   â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ Save in workflow_policies[C]
          â–¼
```

---
## ğŸ’¡ Why Workflow-Specific Storage is Better

### **Benefits of This Approach:**

1. **Accurate Policy Per Workflow**
   - Each workflow gets its own optimized policy
   - No contamination from other workflows
   - True performance measurement per workflow

2. **Resume Capability**
   - If GP-UCB re-selects a workflow, resume from where you left off
   - Allows incremental improvement of promising workflows
   - Can train workflows to higher compliance over time

3. **Fair GP-UCB Evaluation**
   - Each workflow evaluated on its OWN merits
   - Not biased by previous workflow's learning
   - GP learns true workflow quality, not transfer effects

4. **Clear Semantics**
   - "New workflow" = train from scratch
   - "Same workflow" = resume previous training
   - Easy to understand and debug

### **Example Scenarios:**

**Scenario 1: All New Workflows**
```
Iteration 1: Workflow A â†’ Train from scratch (300 episodes to 95%)
Iteration 2: Workflow B â†’ Warm start from A (220 episodes to 95%)
Iteration 3: Workflow C â†’ Warm start from the closest prior workflow (250 episodes to 95%)
...
Total: Each workflow benefits from nearest-neighbour warm starts
```

**Scenario 2: GP Re-selects a Promising Workflow**
```
Iteration 1: Workflow A â†’ Train from scratch (300 episodes, 85% compliance)
Iteration 2: Workflow B â†’ Train from scratch (300 episodes, 90% compliance)
Iteration 3: Workflow B â†’ Inherit from Iteration 2! (50 episodes, 95% compliance âœ“)
                         â””â”€ GP found B is good, continues training it!
```

**Scenario 3: Mix of New and Revisited**
```
Iteration 1: Workflow A â†’ New (300 eps, 88% compliance)
Iteration 2: Workflow B â†’ New (warm start, 260 eps, 92% compliance)  
Iteration 3: Workflow C â†’ New (warm start, 240 eps, 85% compliance)
Iteration 4: Workflow B â†’ Resume (50 eps, 95% âœ“) â† GP focuses on best
Iteration 5: Workflow D â†’ New (300 eps, 75% compliance)
Iteration 6: Workflow B â†’ Resume (0 eps, 95% âœ“) â† Already achieved!
```

---

## ğŸ”§ Technical Details

### **What Gets Inherited (When Revisiting Same Workflow):**

```python
workflow_key = tuple(workflow_order)

if workflow_key in self.workflow_policies:
    # Load THIS workflow's previous policy
    agent.policy.load_state_dict(self.workflow_policies[workflow_key].policy.state_dict())
    agent.policy_old.load_state_dict(self.workflow_policies[workflow_key].policy_old.state_dict())
```

**Inherited (when revisiting):**
- âœ… Actor network weights for THIS workflow
- âœ… Critic network weights for THIS workflow
- âœ… All learned parameters from THIS workflow's previous training

**NOT Inherited:**
- âŒ Policies from OTHER workflows (each workflow independent!)
- âŒ Optimizer state (reset each time)
- âŒ Compliance counters (reset)
- âŒ Episode buffers (cleared)

### **The Policy Network Structure:**

```
Input: [State (52 dims) + Workflow Encoding (25 dims)] = 77 dims
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
  Actor    Critic
    â”‚         â”‚
    â–¼         â–¼
 Actions   Value
```

**Key Point:** The workflow encoding is PART of the input, so:
- Different workflows â†’ Different inputs â†’ Different optimal policies
- Each workflow should have its own policy
- No reason to inherit from a different workflow!

---

## ğŸ“ Example Training Progression (New Behavior)

### First Training of Workflow A: `user â†’ op_host â†’ op_server â†’ enterprise â†’ defender`
```
Iteration 1:
  Creating new agent (new workflow - training from scratch)
  
  Update 1:  Compliance: 56.33% (learning from scratch)
  Update 2:  Compliance: 60.13% (improving)
  Update 3:  Compliance: 68.50%
  Update 4:  Compliance: 75.00%
  ...
  Update 10: Compliance: 95.00% âœ“ (achieved after 250 episodes)
  
  Policy saved in workflow_policies[('user', 'op_host', ...)]
```

### First Training of Workflow B: `defender â†’ enterprise â†’ op_server â†’ op_host â†’ user`
```
Iteration 2:
  Creating new agent (new workflow - training from scratch)
  
  Update 1:  Compliance: 58.00% (learning from scratch - independent of Workflow A!)
  Update 2:  Compliance: 65.00%
  ...
  Update 8:  Compliance: 95.50% âœ“ (achieved after 200 episodes)
  
  Policy saved in workflow_policies[('defender', 'enterprise', ...)]
```

### Re-visiting Workflow A (GP found it promising!)
```
Iteration 5:
  Inheriting policy from previous training of THIS workflow
  (Resuming from checkpoint for this specific workflow)
  
  Update 1:  Compliance: 95.00% (already achieved!) âœ“
  
  No more training needed - already compliant!
```

---

## ğŸ”¬ What if You Want to Disable Resume Capability?

If you want to ALWAYS train from scratch (even when revisiting workflows):

```python
# In train_workflow_async():
workflow_key = tuple(workflow_order)

# Comment out the inheritance check:
# if workflow_key in self.workflow_policies:
#     agent.policy.load_state_dict(...)

# Always create new agent
agent = ParallelOrderConditionedPPO(...)
```

**Effect:**
- Every iteration trains from scratch
- No resume capability
- Each workflow visit is independent
- Useful for testing if GP-UCB re-selection helps

**Current Behavior (Recommended):**
- New workflows: Train from scratch âœ…
- Revisited workflows: Resume from previous training âœ…
- Best of both worlds!

---

## ğŸ“Š Impact on Episode Budget

With 100,000 episode budget:

### **Current Behavior (Workflow-Specific Storage):**
```
Iteration 1: Workflow A â†’ 250 episodes â†’ 95% compliance
Iteration 2: Workflow B â†’ 300 episodes â†’ 95% compliance
Iteration 3: Workflow C â†’ 280 episodes â†’ 95% compliance
Iteration 4: Workflow B â†’ 0 episodes â†’ 95% compliance (resume, already compliant!)
Iteration 5: Workflow D â†’ 270 episodes â†’ 95% compliance
...

Unique workflows trained: ~300-350
Workflow re-visits: Minimal episodes (already trained)
Total coverage: Good exploration of search space
```

### **Advantages:**

1. **Fair Evaluation**
   - Each workflow measured on its own performance
   - No transfer effects confounding GP-UCB
   
2. **Efficient Re-selection**
   - If GP picks a good workflow again, resume instantly
   - No wasted episodes re-training compliant workflows

3. **Clean Workflow Comparison**
   - Can directly compare workflow rewards
   - Each trained independently
   - True workflow quality discovered

---

## âœ… Summary

**Each workflow gets its own policy!** The system now works as follows:

1. **New workflow**: Train from scratch (no inheritance from other workflows)
2. **Same workflow revisited**: Resume from that workflow's previous checkpoint
3. **Result**: Fair evaluation of each workflow + efficient re-selection
4. **Benefit**: True workflow quality measured, no cross-workflow contamination

**Key Data Structure:**
```python
workflow_policies = {
    ('user', 'op_host', 'op_server', 'enterprise', 'defender'): Policy_A,
    ('defender', 'enterprise', 'op_server', 'op_host', 'user'): Policy_B,
    ('op_server', 'defender', 'enterprise', 'user', 'op_host'): Policy_C,
    ...
}
```

This is the **correct approach** for workflow search - each workflow evaluated independently!

---

## ğŸ¯ Key Code Locations

### Executor Async (lines 341-385):
```python
# Check if THIS specific workflow was trained before
workflow_key = tuple(workflow_order)

if workflow_key in self.workflow_policies:
    # Resume from THIS workflow's checkpoint
    print("  Inheriting policy from previous training of THIS workflow")
    agent = ParallelOrderConditionedPPO(...)
    agent.policy.load_state_dict(self.workflow_policies[workflow_key].policy.state_dict())
else:
    # New workflow - create base agent, then warm start from the closest prior workflow (if any)
    print("  Creating new agent (new workflow - training from scratch)")
    agent = ParallelOrderConditionedPPO(...)
    closest_key, closest_agent, closest_distance = self._find_closest_trained_workflow(workflow_order)
    if closest_agent is not None:
        closest_order = ' â†’ '.join(list(closest_key))
        print(f\"  Initializing from closest trained workflow: {closest_order} (dist={closest_distance:.3f})\")
        agent.policy.load_state_dict(closest_agent.policy.state_dict())
        agent.policy_old.load_state_dict(closest_agent.policy_old.state_dict())
```

### After training (line 496):
```python
# Save agent for THIS specific workflow
workflow_key = tuple(workflow_order)
self.workflow_policies[workflow_key] = agent  # â† Store per workflow!
```

### Storage Structure (line 215):
```python
# Dictionary mapping workflow to its trained policy
self.workflow_policies = {}  # {workflow_tuple: agent}
```

This pattern provides:
- âœ… Workflow-specific policies
- âœ… Resume capability for promising workflows
- âœ… Warm starts for unseen workflows (faster compliance ramp-up)
- âœ… Fair evaluation (no cross-workflow contamination)
